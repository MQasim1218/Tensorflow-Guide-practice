{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **AutoDiff**\n",
    "Automatic differentiation is useful for implementing machine learning algorithms such as backpropagation for training neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Computing Gradients**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Gradient Tapes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a function for comoutation\n",
    "def fn(x_ten):\n",
    "    return x_ten**2\n",
    "\n",
    "x = tf.Variable(3.0)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    # Use defined function to transform the input.\n",
    "    y = fn(x)\n",
    "\n",
    "# Use GradientTape() to compute gradient... \n",
    "tape.gradient(y, x).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Gradients with nd-Variables\n",
    "v1 = tf.Variable(tf.random.normal([3,3], 3, 9,seed= 3, name= 'var1'))\n",
    "v2 = tf.Variable(tf.eye(3), name='var2')\n",
    "\n",
    "w = [[1. ,2. ,3. ]]\n",
    "\n",
    "# Recording operations onto `tape`\n",
    "with tf.GradientTape() as tape:\n",
    "    y = v1 * w + v2\n",
    "    loss = tf.reduce_mean(y**2)\n",
    "    \n",
    "# Computing gradients w.r.t...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Using a dictionary of Variables with the same tape as above**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(dl_dv1, dl_dv2) = tape.gradient(loss, [v1, v2])\n",
    "  \n",
    "# np.argsort organizes the index bsaed on the sorted order of the tensor.\n",
    "np.argsort(tf.nn.softmax(dl_dv1.numpy()))\n",
    "dl_dv2.numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dl_dw': <tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
      "array([[  0.5811825 , -10.723121  ,   6.628027  ],\n",
      "       [  2.0538814 ,   8.9651165 ,  38.633232  ],\n",
      "       [  3.0487926 ,   0.34223557, -26.612106  ]], dtype=float32)>, 'dl_db': <tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
      "array([[ 0.5811825 , -5.3615603 ,  2.2093422 ],\n",
      "       [ 2.0538814 ,  4.4825583 , 12.877744  ],\n",
      "       [ 3.0487926 ,  0.17111778, -8.870702  ]], dtype=float32)>}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
       "array([[ 0.5811825 , -5.3615603 ,  2.2093422 ],\n",
       "       [ 2.0538814 ,  4.4825583 , 12.877744  ],\n",
       "       [ 3.0487926 ,  0.17111778, -8.870702  ]], dtype=float32)>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_vars = {\n",
    "    'dl_dw': v1,\n",
    "    'dl_db': v2\n",
    "}\n",
    "\n",
    "grad = tape.gradient(loss, my_vars)\n",
    "\n",
    "for i in grad:\n",
    "    print(f'The name of the tensor-Variable is: {i}')\n",
    "    print(grad[i].numpy())\n",
    "    print()\n",
    "\n",
    "np.argsort(tf.nn.softmax(grad['dl_db']).numpy())\n",
    "\n",
    "# We discovered a new problem here...  \n",
    "# If this problem occurs \n",
    "# 'A non-persistent GradientTape can only be used to compute one set of gradients (or jacobians)'\n",
    "# It means the tape has previously been used to compute the gradients, thus it can not be reused.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Model Gradients**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow.keras.layers as lyrs\n",
    "import tensorflow.keras.activations as act\n",
    "\n",
    "D_layer = lyrs.Dense(32, activation= act.relu)\n",
    "x_ten = tf.cast(tf.linspace([1,5], [10, -4], 10), tf.float16)\n",
    "x_ten\n",
    "\n",
    "with tf.GradientTape() as tape2:\n",
    "    y = D_layer(x_ten)\n",
    "    loss = tf.reduce_mean(y ** 2)\n",
    "# Computing the gradient    \n",
    "grad = tape2.gradient(loss, D_layer.trainable_variables)\n",
    "\n",
    "# Displaying the Output\n",
    "tf.nn.softmax(grad[0]).numpy()\n",
    "np.argmax(tf.nn.softmax(grad[0]).numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense_23/kernel:0, shape: (2, 32)\n",
      "dense_23/bias:0, shape: (32,)\n"
     ]
    }
   ],
   "source": [
    "for var, g in zip(D_layer.trainable_variables, grad):\n",
    "  print(f'{var.name}, shape: {g.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Controlling what the tape watches**\n",
    "> The default behavior is to record all operations after accessing a **`trainable tf.Variable`**.\n",
    "\n",
    "> Tape cannot compute gradients for tesnors as thet're immutables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(6.0, shape=(), dtype=float32)\n",
      "None\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# The only trainable Variable\n",
    "var1 = tf.Variable(3.0)\n",
    "# Non-trainanle\n",
    "var2 = tf.Variable(5.0, trainable= False)\n",
    "\n",
    "# A constant tensor (no grads for tensors)\n",
    "ten1 = tf.constant(5.0)\n",
    "# Variable + constant = constant, thus below is also a tensor\n",
    "ten2 = tf.Variable(10.0) + 3.0\n",
    "\n",
    "with tf.GradientTape() as tape3:\n",
    "    y = (var1**2) + (var2**2) + (ten1**2) + ten2\n",
    "\n",
    "grads = tape3.gradient(y, [var1, var2, ten1, ten2])\n",
    "\n",
    "for gr in grads:\n",
    "    print(gr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List Variables watched by the tape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=3.0>]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Variable:0']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print([var for var in tape3.watched_variables()])\n",
    "[var.name for var in tape3.watched_variables()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **To record gradients with respect to a tf.Tensor, you need to call `GradientTape.watch(x)`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[842.26544 154.64954 706.1339 ]\n",
      " [926.0429  298.74033 326.2389 ]\n",
      " [845.9513  426.66037 588.9402 ]], shape=(3, 3), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]], shape=(3, 3), dtype=float32)\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "rten = tf.random.uniform([3, 3], 0, 20, seed= 3)\n",
    "\n",
    "tf.transpose(rten)\n",
    "\n",
    "with tf.GradientTape() as ten_tape:\n",
    "    ten_tape.watch(rten)\n",
    "    y = rten @ (tf.transpose(rten) *  tf.random.normal([3, 3], 10, 5))\n",
    "\n",
    "dy_dx = ten_tape.gradient(y, rten)\n",
    "print(dy_dx)\n",
    "print(tf.nn.softmax(dy_dx))\n",
    "print(np.argmax(tf.nn.softmax(dy_dx)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **To disable default behaviuor of Gradient-Tape, use `watch_accessed_variables=False` paramater**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing Gradients for each variable\n",
      "Variable1\n",
      "None\n",
      "\n",
      "Variable2\n",
      "tf.Tensor(43.4, shape=(), dtype=float32)\n",
      "\n",
      "[<tf.Variable 'var2:0' shape=() dtype=float32, numpy=21.7>]\n",
      "This shows that only var2 is being watched\n"
     ]
    }
   ],
   "source": [
    "v1 = tf.Variable(31.4, name= 'var1')\n",
    "v2 = tf.Variable(21.7, name= 'var2')\n",
    "\n",
    "with tf.GradientTape(watch_accessed_variables= False) as gt:\n",
    "    gt.watch(v2)\n",
    "    y = v1**2 + v2**2\n",
    "\n",
    "vars = {\n",
    "    'Variable1' : v1,\n",
    "    'Variable2' : v2\n",
    "}\n",
    "\n",
    "grz = gt.gradient(y, vars)\n",
    "\n",
    "print(\"Printing Gradients for each variable\")\n",
    "for gr in grz:\n",
    "    print(gr)\n",
    "    print(grz[gr])\n",
    "    print()\n",
    "\n",
    "\n",
    "# Printing all watched variables\n",
    "print([var for var in gt.watched_variables()])\n",
    "print(\"This shows that only var2 is being watched\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Intermediate Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xten = tf.constant([1, 3.2, 5.4])\n",
    "\n",
    "with tf.GradientTape() as gt:\n",
    "    gt.watch(xten)\n",
    "    y = x ** 2\n",
    "    z = y * x\n",
    "\n",
    "gt.gradient(z, y).numpy() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Note on Performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Gradients of non-scalar targets**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Control Flow**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Getting Gradient of None**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **No Gradient registered**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Zeros instead of None**"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "445be380edc556d8a8859931574c9be2b357dc49fbb96280944087ec4ff5e718"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('OCR': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
