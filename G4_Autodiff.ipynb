{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **AutoDiff**\n",
    "Automatic differentiation is useful for implementing machine learning algorithms such as backpropagation for training neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Computing Gradients**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Gradient Tapes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a function for comoutation\n",
    "def fn(x_ten):\n",
    "    return x_ten**2\n",
    "\n",
    "x = tf.Variable(3.0)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    # Use defined function to transform the input.\n",
    "    y = fn(x)\n",
    "\n",
    "# Use GradientTape() to compute gradient... \n",
    "tape.gradient(y, x).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Gradients with nd-Variables\n",
    "v1 = tf.Variable(tf.random.normal([3,3], 3, 9,seed= 3, name= 'var1'))\n",
    "v2 = tf.Variable(tf.eye(3), name='var2')\n",
    "\n",
    "w = [[1. ,2. ,3. ]]\n",
    "\n",
    "# Recording operations onto `tape`\n",
    "with tf.GradientTape() as tape:\n",
    "    y = v1 * w + v2\n",
    "    loss = tf.reduce_mean(y**2)\n",
    "    \n",
    "# Computing gradients w.r.t...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Using a dictionary of Variables with the same tape as above**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(dl_dv1, dl_dv2) = tape.gradient(loss, [v1, v2])\n",
    "  \n",
    "# np.argsort organizes the index bsaed on the sorted order of the tensor.\n",
    "np.argsort(tf.nn.softmax(dl_dv1.numpy()))\n",
    "dl_dv2.numpy()\n",
    "\n",
    "np.argsort(tf.nn.softmax(grad['dl_db']).numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dl_dw': <tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
      "array([[  0.5811825 , -10.723121  ,   6.628027  ],\n",
      "       [  2.0538814 ,   8.9651165 ,  38.633232  ],\n",
      "       [  3.0487926 ,   0.34223557, -26.612106  ]], dtype=float32)>, 'dl_db': <tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
      "array([[ 0.5811825 , -5.3615603 ,  2.2093422 ],\n",
      "       [ 2.0538814 ,  4.4825583 , 12.877744  ],\n",
      "       [ 3.0487926 ,  0.17111778, -8.870702  ]], dtype=float32)>}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
       "array([[ 0.5811825 , -5.3615603 ,  2.2093422 ],\n",
       "       [ 2.0538814 ,  4.4825583 , 12.877744  ],\n",
       "       [ 3.0487926 ,  0.17111778, -8.870702  ]], dtype=float32)>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_vars = {\n",
    "    'dl_dw': v1,\n",
    "    'dl_db': v2\n",
    "}\n",
    "\n",
    "grad = tape.gradient(loss, my_vars)\n",
    "\n",
    "for i in grad:\n",
    "    print(f'The name of the tensor-Variable is: {i}')\n",
    "    print(grad[i].numpy())\n",
    "    print()\n",
    "\n",
    "# We discovered a new problem here...  \n",
    "# If this problem occuers \n",
    "# 'A non-persistent GradientTape can only be used to compute one set of gradients (or jacobians)'\n",
    "# It means the tape has previously been used to compute the gradients, thus it can not be reused.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Model Gradients**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Controlling what the tape watches**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Intermediate Results**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Note on Performance**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Gradients of non-scalar targets**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Control Flow**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Getting Gradient of None**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **No Gradient registered**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Zeros instead of None**"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "445be380edc556d8a8859931574c9be2b357dc49fbb96280944087ec4ff5e718"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('OCR': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
