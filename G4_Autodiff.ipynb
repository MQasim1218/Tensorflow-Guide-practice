{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **AutoDiff**\n",
    "Automatic differentiation is useful for implementing machine learning algorithms such as backpropagation for training neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Computing Gradients**\n",
    "TensorFlow uses GradientTape function to compute the Gradients (loss fn, other values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Gradient Tapes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a function for comoutation\n",
    "def fn(x_ten):\n",
    "    return x_ten**2\n",
    "\n",
    "x = tf.Variable(3.0)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    # Use defined function to transform the input.\n",
    "    y = fn(x)\n",
    "\n",
    "# Use GradientTape() to compute gradient... \n",
    "tape.gradient(y, x).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Gradients with nd-Variables\n",
    "v1 = tf.Variable(tf.random.normal([3,3], 3, 9,seed= 3, name= 'var1'))\n",
    "v2 = tf.Variable(tf.eye(3), name='var2')\n",
    "\n",
    "w = [[1. ,2. ,3. ]]\n",
    "\n",
    "# Recording operations onto `tape`\n",
    "with tf.GradientTape() as tape:\n",
    "    y = v1 * w + v2\n",
    "    loss = tf.reduce_mean(y**2)\n",
    "    \n",
    "# Computing gradients w.r.t...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Using a dictionary of Variables with the same tape as above**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(dl_dv1, dl_dv2) = tape.gradient(loss, [v1, v2])\n",
    "  \n",
    "# np.argsort organizes the index bsaed on the sorted order of the tensor.\n",
    "np.argsort(tf.nn.softmax(dl_dv1.numpy()))\n",
    "dl_dv2.numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dl_dw': <tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
      "array([[  0.5811825 , -10.723121  ,   6.628027  ],\n",
      "       [  2.0538814 ,   8.9651165 ,  38.633232  ],\n",
      "       [  3.0487926 ,   0.34223557, -26.612106  ]], dtype=float32)>, 'dl_db': <tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
      "array([[ 0.5811825 , -5.3615603 ,  2.2093422 ],\n",
      "       [ 2.0538814 ,  4.4825583 , 12.877744  ],\n",
      "       [ 3.0487926 ,  0.17111778, -8.870702  ]], dtype=float32)>}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
       "array([[ 0.5811825 , -5.3615603 ,  2.2093422 ],\n",
       "       [ 2.0538814 ,  4.4825583 , 12.877744  ],\n",
       "       [ 3.0487926 ,  0.17111778, -8.870702  ]], dtype=float32)>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_vars = {\n",
    "    'dl_dw': v1,\n",
    "    'dl_db': v2\n",
    "}\n",
    "\n",
    "grad = tape.gradient(loss, my_vars)\n",
    "\n",
    "for i in grad:\n",
    "    print(f'The name of the tensor-Variable is: {i}')\n",
    "    print(grad[i].numpy())\n",
    "    print()\n",
    "\n",
    "np.argsort(tf.nn.softmax(grad['dl_db']).numpy())\n",
    "\n",
    "# We discovered a new problem here...  \n",
    "# If this problem occurs \n",
    "# 'A non-persistent GradientTape can only be used to compute one set of gradients (or jacobians)'\n",
    "# It means the tape has previously been used to compute the gradients, thus it can not be reused.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Model Gradients**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow.keras.layers as lyrs\n",
    "import tensorflow.keras.activations as act\n",
    "\n",
    "D_layer = lyrs.Dense(32, activation= act.relu)\n",
    "x_ten = tf.cast(tf.linspace([1,5], [10, -4], 10), tf.float16)\n",
    "x_ten\n",
    "\n",
    "with tf.GradientTape() as tape2:\n",
    "    y = D_layer(x_ten)\n",
    "    loss = tf.reduce_mean(y ** 2)\n",
    "# Computing the gradient    \n",
    "grad = tape2.gradient(loss, D_layer.trainable_variables)\n",
    "\n",
    "# Displaying the Output\n",
    "tf.nn.softmax(grad[0]).numpy()\n",
    "np.argmax(tf.nn.softmax(grad[0]).numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense_23/kernel:0, shape: (2, 32)\n",
      "dense_23/bias:0, shape: (32,)\n"
     ]
    }
   ],
   "source": [
    "for var, g in zip(D_layer.trainable_variables, grad):\n",
    "  print(f'{var.name}, shape: {g.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Controlling what the tape watches**\n",
    "> The default behavior is to record all operations after accessing a **`trainable tf.Variable`**.\n",
    "\n",
    "> Tape cannot compute gradients for tesnors as thet're immutables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(6.0, shape=(), dtype=float32)\n",
      "None\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# The only trainable Variable\n",
    "var1 = tf.Variable(3.0)\n",
    "# Non-trainanle\n",
    "var2 = tf.Variable(5.0, trainable= False)\n",
    "\n",
    "# A constant tensor (no grads for tensors)\n",
    "ten1 = tf.constant(5.0)\n",
    "# Variable + constant = constant, thus below is also a tensor\n",
    "ten2 = tf.Variable(10.0) + 3.0\n",
    "\n",
    "with tf.GradientTape() as tape3:\n",
    "    y = (var1**2) + (var2**2) + (ten1**2) + ten2\n",
    "\n",
    "grads = tape3.gradient(y, [var1, var2, ten1, ten2])\n",
    "\n",
    "for gr in grads:\n",
    "    print(gr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List Variables watched by the tape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=3.0>]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Variable:0']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print([var for var in tape3.watched_variables()])\n",
    "[var.name for var in tape3.watched_variables()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **To record gradients with respect to a tf.Tensor, you need to call `GradientTape.watch(x)`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[842.26544 154.64954 706.1339 ]\n",
      " [926.0429  298.74033 326.2389 ]\n",
      " [845.9513  426.66037 588.9402 ]], shape=(3, 3), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]], shape=(3, 3), dtype=float32)\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "rten = tf.random.uniform([3, 3], 0, 20, seed= 3)\n",
    "\n",
    "tf.transpose(rten)\n",
    "\n",
    "with tf.GradientTape() as ten_tape:\n",
    "    ten_tape.watch(rten)\n",
    "    y = rten @ (tf.transpose(rten) *  tf.random.normal([3, 3], 10, 5))\n",
    "\n",
    "dy_dx = ten_tape.gradient(y, rten)\n",
    "print(dy_dx)\n",
    "print(tf.nn.softmax(dy_dx))\n",
    "print(np.argmax(tf.nn.softmax(dy_dx)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **To disable default behaviuor of Gradient-Tape, use `watch_accessed_variables=False` paramater**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing Gradients for each variable\n",
      "Variable1\n",
      "None\n",
      "\n",
      "Variable2\n",
      "tf.Tensor(43.4, shape=(), dtype=float32)\n",
      "\n",
      "[<tf.Variable 'var2:0' shape=() dtype=float32, numpy=21.7>]\n",
      "This shows that only var2 is being watched\n"
     ]
    }
   ],
   "source": [
    "v1 = tf.Variable(31.4, name= 'var1')\n",
    "v2 = tf.Variable(21.7, name= 'var2')\n",
    "\n",
    "with tf.GradientTape(watch_accessed_variables= False) as gt:\n",
    "    gt.watch(v2)\n",
    "    y = v1**2 + v2**2\n",
    "\n",
    "vars = {\n",
    "    'Variable1' : v1,\n",
    "    'Variable2' : v2\n",
    "}\n",
    "\n",
    "grz = gt.gradient(y, vars)\n",
    "\n",
    "print(\"Printing Gradients for each variable\")\n",
    "for gr in grz:\n",
    "    print(gr)\n",
    "    print(grz[gr])\n",
    "    print()\n",
    "\n",
    "\n",
    "# Printing all watched variables\n",
    "print([var for var in gt.watched_variables()])\n",
    "print(\"This shows that only var2 is being watched\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Intermediate Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1. , 3.2, 5.4], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xten = tf.constant([1, 3.2, 5.4])\n",
    "\n",
    "with tf.GradientTape() as gt:\n",
    "    gt.watch(xten)\n",
    "    y = xten ** 2\n",
    "    z = y * xten\n",
    "\n",
    "gt.gradient(z, y).numpy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the main array\n",
      " [[[ 5.5       9.458824]\n",
      "  [13.417647 17.37647 ]\n",
      "  [21.335295 25.294119]]\n",
      "\n",
      " [[29.252941 33.211765]\n",
      "  [37.17059  41.129414]\n",
      "  [45.088238 49.047062]]\n",
      "\n",
      " [[53.005882 56.964706]\n",
      "  [60.92353  64.882355]\n",
      "  [68.84118  72.8     ]]]\n",
      "\n",
      "[[[-5.39861    5.39861  ]\n",
      "  [ 9.724726  -9.724726 ]\n",
      "  [ 7.136075  -7.136075 ]]\n",
      "\n",
      " [[-3.386214   3.3862143]\n",
      "  [ 1.8387884 -1.8387884]\n",
      "  [ 0.5631099 -0.5631094]]\n",
      "\n",
      " [[-3.006029   3.0060282]\n",
      "  [ 5.3085713 -5.3085713]\n",
      "  [ 8.977145  -8.977145 ]]]\n",
      "\n",
      "[[[ 0.0000000e+00 -2.8246850e-01]\n",
      "  [-0.0000000e+00  6.2194556e-02]\n",
      "  [-0.0000000e+00 -6.5302171e-02]]\n",
      "\n",
      " [[ 0.0000000e+00 -2.8457141e-01]\n",
      "  [-2.1457661e-06 -4.3686420e-02]\n",
      "  [-2.6601760e-02 -2.7351511e-01]]\n",
      "\n",
      " [[ 0.0000000e+00  2.9256722e-01]\n",
      "  [-0.0000000e+00  2.7639383e-01]\n",
      "  [-0.0000000e+00  2.7315727e-02]]]\n",
      "\n",
      "[[[ 0.04064255 -0.0406425 ]\n",
      "  [ 0.0019184  -0.0019184 ]\n",
      "  [ 0.00211517 -0.00211518]]\n",
      "\n",
      " [[ 0.0412682  -0.04126835]\n",
      "  [ 0.0009419  -0.0009419 ]\n",
      "  [ 0.01935778 -0.01935792]]\n",
      "\n",
      " [[ 0.04369489 -0.04369497]\n",
      "  [ 0.03886445 -0.03886461]\n",
      "  [ 0.00036966 -0.00036966]]]\n"
     ]
    }
   ],
   "source": [
    "ten1 = tf.cast(tf.linspace(5.5, 72.8, 18), tf.float32)\n",
    "ten1 = tf.reshape(ten1, shape=(3,3 , -1))\n",
    "print(\"This is the main array\\n\", ten1.numpy())\n",
    "\n",
    "def f(x):\n",
    "    return tf.nn.log_softmax(x) * 1.5 * tf.random.normal(shape=[3,3,2], mean=0, stddev= 5)\n",
    "\n",
    "def g(x):\n",
    "    return tf.tanh(x) ** 2\n",
    "\n",
    "with tf.GradientTape(persistent= True) as tx:\n",
    "    tx.watch(ten1)\n",
    "    y1 = f(ten1)\n",
    "    y2 = g(y1)\n",
    "\n",
    "print()    \n",
    "print(tx.gradient(y1, ten1).numpy())\n",
    "print()\n",
    "\n",
    "print()\n",
    "print(tx.gradient(y2, y1).numpy())\n",
    "\n",
    "print()\n",
    "print(tx.gradient(y2, ten1).numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Note on Performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## There is a tiny overhead associated with doing operations inside a gradient tape context. \n",
    "#  For most eager execution this will not be a noticeable cost, but you should still use \n",
    "#  tape context around the areas only where it is required.\n",
    "\n",
    "## Gradient tapes use memory to store intermediate results, including inputs and outputs, \n",
    "#  for use during the backwards pass.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Gradients of non-scalar targets**\n",
    "> **MultiGradients**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient(dy1_dx): [ 2.  8. 18.]\n",
      "Gradient(dy2_dx): [-2.         -0.03125    -0.00274348]\n",
      "Composite Gradients\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([1., 4., 9.])\n",
    "\n",
    "with tf.GradientTape(persistent=True) as gt:\n",
    "    gt.watch(x)\n",
    "    y1 = x * x\n",
    "    y2 = 1 / y1\n",
    "\n",
    "# Comuting Individual gradients...\n",
    "g1 = gt.gradient(y1 ,x)\n",
    "g2 = gt.gradient(y2, x)\n",
    "\n",
    "print(f'Gradient(dy1_dx): {g1}')\n",
    "print(f'Gradient(dy2_dx): {g2}')\n",
    "\n",
    "g_com = gt.gradient({'g1': y1, 'g2': y2}, x)\n",
    "\n",
    "print(\"Composite Gradients\")\n",
    "print(g_com.numpy())\n",
    "\n",
    "###  Computing the gradient of multiple targets, the result for each source is:\n",
    "#       The gradient of the sum of the targets, or equivalently\n",
    "#       The sum of the gradients of each target.\n",
    "del gt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## If the target(s) are not scalar the gradient of the sum is calculated\n",
    "x = tf.Variable(2.0)\n",
    "\n",
    "with tf.GradientTape() as gt:\n",
    "    y = x * [3. , 4.]\n",
    "\n",
    "gt.gradient(y, x).numpy()\n",
    "\n",
    "del gt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " > **For an element-wise calculation, the gradient of the sum gives the derivative of each element with respect to its input-element, since each element is independent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = tf.linspace(-10, 10, 101)\n",
    "x.numpy()\n",
    "\n",
    "with tf.GradientTape() as gt:\n",
    "    gt.watch(x)\n",
    "    y = tf.nn.sigmoid(x)\n",
    "    \n",
    "der = gt.gradient(y, x)\n",
    "der \n",
    "\n",
    "pl.plot(x, y, 'b', label= 'y')\n",
    "pl.plot(x, der, 'r', label= 'derivative_y')\n",
    "pl.xlabel('Data-Values')\n",
    "pl.ylabel('Computed-Values')\n",
    "pl.legend()\n",
    "pl.show()\n",
    "\n",
    "del gt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Control Flow**\n",
    "> How the gradients are computed incase of branched operations within the GradientTape (`if/else, loops`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Because a gradient tape records operations as they are executed, \n",
    "# Python control flow is naturally handled\n",
    "\n",
    "x = 0\n",
    "\n",
    "v1 = tf.Variable(3.)\n",
    "v2 = tf.Variable([2.5, -2.5])\n",
    "\n",
    "with tf.GradientTape() as gt:\n",
    "    for i in range(3):\n",
    "        if x == 0:\n",
    "            y1 = v1**2\n",
    "            x = 1\n",
    "        else:\n",
    "            y1 = v2 / v1\n",
    "            x = 0\n",
    "\n",
    "[g1, g2] = gt.gradient(y1, [v1, v2])\n",
    "print(g1.numpy())\n",
    "print(g2)\n",
    "\n",
    "del gt\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Because a gradient tape records operations as they are executed, \n",
    "# Python control flow is naturally handled\n",
    "\n",
    "x = 0\n",
    "\n",
    "v1 = tf.Variable(3.)\n",
    "v2 = tf.Variable([2.5, -2.5])\n",
    "\n",
    "with tf.GradientTape() as gt:\n",
    "    for i in range(3):\n",
    "        if x == 0:\n",
    "            y1 = v1**2\n",
    "            x = 1\n",
    "        else:\n",
    "            y1 = v2 / v1\n",
    "            x = 0\n",
    "\n",
    "[g1, g2] = gt.gradient(y1, [v1, v2])\n",
    "print(g1.numpy())\n",
    "print(g2)\n",
    "\n",
    "del gt\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Getting Gradient of None**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ##### **When a target is not connected to a source you will get a gradient of None.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "x1 = tf.constant(2.)\n",
    "x2 = tf.constant(4.)\n",
    "\n",
    "with tf.GradientTape() as gt:\n",
    "    y = tf.tanh(x1)\n",
    "\n",
    "print(gt.gradient(y, x2)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ##### **Replaced Variable with Tensor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of x ResourceVariable\n",
      "type of x ResourceVariable\n",
      "type of x ResourceVariable\n",
      "type of x EagerTensor\n",
      "tf.Tensor(0.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# One common error is to inadvertently replace a tf.Variable with a tf.Tensor, \n",
    "# instead of using Variable.assign to update the tf.Variable.\n",
    "# Eaxmple\n",
    "x = tf.Variable(4.0)\n",
    "with tf.GradientTape() as gt:\n",
    "    y = 0\n",
    "    for epochs in range(3):\n",
    "        y = tf.nn.sigmoid(x)\n",
    "        print(f'type of x {type(x).__name__}')\n",
    "    \n",
    "    x.assign_add(1.5) # Correct syntax\n",
    "    x = x + 1 # Here, x converts to a tensor\n",
    "\n",
    "    print(f'type of x {type(x).__name__}')\n",
    "\n",
    "g = gt.gradient(y, x)\n",
    "print(g)\n",
    "\n",
    "del gt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ##### **Working with Varibles out of TensorFlow**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable([\n",
    "    [5,6,8,3,5],\n",
    "    [9,8,7,1,34]\n",
    "], dtype=tf.float32)\n",
    "\n",
    "with tf.GradientTape() as gt:\n",
    "    y = x*2\n",
    "    \n",
    "    y = np.mean(y, axis=0) # Numpy calculation\n",
    "    \n",
    "    y = tf.reduce_mean(y, axis= 0)\n",
    "\n",
    "print(gt.gradient(y, x))\n",
    "del gt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ##### **Gradients of an int or string is None**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.int32\n",
      "WARNING:tensorflow:The dtype of the target tensor must be floating (e.g. tf.float32) when calling GradientTape.gradient, got tf.int32\n",
      "WARNING:tensorflow:The dtype of the source tensor must be floating (e.g. tf.float32) when calling GradientTape.gradient, got tf.int32\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant(10)\n",
    "with tf.GradientTape() as gt:\n",
    "    gt.watch(x)\n",
    "    y = tf.transpose(x)\n",
    "\n",
    "print(gt.gradient(y,x))\n",
    "del gt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ##### **Taking Gradients through a stateful object**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=float32, numpy=-4.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=2.0>)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## State stops gradients. When you read from a stateful object, the tape can only observe \n",
    "#  the current state, not the history that lead to it.\n",
    "\n",
    "## A tf.Variable has internal stateâ€”its value. When you use the variable, the state is read. \n",
    "#  It's normal to calculate a gradient with respect to a variable, but the variable's state \n",
    "#  blocks gradient calculations from going farther back.\n",
    "\n",
    "a = tf.Variable(1.0)\n",
    "b = tf.Variable(3.0)\n",
    "\n",
    "with tf.GradientTape() as gt:\n",
    "    # The state of the Variable changes here\n",
    "    b.assign_sub(a)\n",
    "    # Tape Starts recording here\n",
    "    y = b * 2\n",
    "    y = y / a\n",
    "    \n",
    "d1, d2 = gt.gradient(y, [a,b])\n",
    "d1, d2    \n",
    "\n",
    "# For some reason, I'm not getting a `None` gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of x ResourceVariable\n",
      "type of x ResourceVariable\n",
      "type of x ResourceVariable\n",
      "type of x EagerTensor\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# One common error is to inadvertently replace a tf.Variable with a tf.Tensor, \n",
    "# instead of using Variable.assign to update the tf.Variable.\n",
    "\n",
    "# Eaxmple\n",
    "x = tf.Variable(4.0)\n",
    "with tf.GradientTape() as gt:\n",
    "    y = 0\n",
    "    for epochs in range(3):\n",
    "        y = tf.nn.sigmoid(x)\n",
    "        print(f'type of x {type(x).__name__}')\n",
    "        \n",
    "    x = x + 1 # Here, x converts to a tensor\n",
    "    print(f'type of x {type(x).__name__}')\n",
    "\n",
    "g = gt.gradient(y, x)\n",
    "print(g)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **No Gradient registered**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LookupError :: gradient registry has no entry for: AdjustContrastv2\n"
     ]
    }
   ],
   "source": [
    "img = tf.constant([\n",
    "    [[0.5, 0., 0.]]\n",
    "])\n",
    "delta = tf.Variable(.20)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "  new_image = tf.image.adjust_contrast(img, delta)\n",
    "\n",
    "try:\n",
    "    tape.gradient(new_image, [img, delta])\n",
    "except LookupError as e:\n",
    "    print(f'{type(e).__name__} :: {e}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Zeros instead of None**\n",
    "> **In some cases it would be convenient to get `0 instead of None for unconnected gradients`. You can decide what to return when you have unconnected gradients using the `unconnected_gradients` argument**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Normal case, Normally a None gradient shall be returned as z and x aren't connected!!\n",
      "None \n",
      "\n",
      "Using unconnected_gradients argument!!\n",
      "tf.Tensor([0. 0.], shape=(2,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable([2., 2.])\n",
    "y = tf.Variable(3.)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "  z = y**2\n",
    "\n",
    "print(\"In Normal case, Normally a None gradient shall be returned as z and x aren't connected!!\")\n",
    "print(tape.gradient(z, x),'\\n')\n",
    "\n",
    "print('Using unconnected_gradients argument!!')\n",
    "print(tape.gradient(z, x, unconnected_gradients=tf.UnconnectedGradients.ZERO))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "445be380edc556d8a8859931574c9be2b357dc49fbb96280944087ec4ff5e718"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('OCR': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
